ğŸ§© How Language Models Work
Language models are AI systems trained to understand and
generate human language. 
Their capabilities rely on a few critical components:

1. Tokenization â€“ Turning Text into Numbers
- Text is split into tokens (words or subwords).
- Common or unimportant words (like â€œtheâ€, â€œaâ€) might be removed (stop word removal).
- Each token is mapped to a unique number so the model can process it.

ğŸ”  Example:
"Van Gogh was a painter" â†’ [Van, Gogh, was, a, painter] â†’ [123, 456, 789, 12, 345]

2. Word Embeddings â€“ Capturing Meaning and Relationships
- Tokens are converted into vectors (multidimensional arrays of numbers).
- These vectors are placed in an embedding space where similar meanings are closer together.

ğŸ“ Example:
â€œpuppyâ€ and â€œdogâ€ might point in similar directions, showing theyâ€™re semantically related.

3. Model Architectures â€“ Understanding Context

ğŸ§  Recurrent Neural Networks (RNNs)
- Process one token at a time, remembering past inputs via a hidden state.
- Struggles with long sentencesâ€”important info (like a subject at the beginning) can get "forgotten" or diluted.

ğŸ› ï¸ Limitations of RNNs
- Hard to prioritize relevant info.
- Performance degrades as sentences get longer.

ğŸ§¬ Why Does This Matter?
Each developmentâ€”tokenization, embeddings, and improved architecturesâ€”allowed 
language models to move from understanding simple patterns to 
truly understanding context, relationships, and meaning.
