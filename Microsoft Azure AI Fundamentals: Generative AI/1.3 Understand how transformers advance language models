🚀 Why Transformers Are a Game-Changer
Transformers replaced the limitations of Recurrent Neural Networks (RNNs) by using attention mechanisms. 
This allows them to:
- Understand context better (even from far apart in the sentence).
- Process text in parallel, making training and inference faster.

🧠 Key Components of a Transformer
1. Encoder–Decoder Architecture
- Encoder: Processes the input and builds a context-aware representation.
- Decoder: Uses that context to generate or predict the output sequence.

Modern language models like GPT use only the decoder part (optimized for generation), 
while BERT uses only the encoder part (optimized for understanding).

2. Positional Encoding
- Since Transformers don't process data sequentially, they need positional encoding to retain word order.
- Combines word embedding with position vectors to encode "where" a word appears in a sentence.

3. Attention Mechanism (Self-Attention)
- Enables the model to focus on the most relevant parts of the input.
- Each word is treated as a query, compared to keys, and retrieves corresponding values.
- Uses scaled dot-product + softmax to find the most relevant keys.

Example:
In the sentence "Van Gogh was a painter...", the word “painter” is the 
key concept that helps define “Van Gogh” — attention helps the model make that link.

4. Multi-Head Attention
- Instead of a single attention process, the model applies multiple in parallel.
- Each head learns different types of relationships, such as syntax, semantics, or positional patterns.

💡 What This Enables
Thanks to Transformers:
- Models can handle long-range dependencies better than RNNs.
- Parallel processing makes training on huge datasets feasible.
- Outputs are context-aware, generating high-quality language, images, or code.

