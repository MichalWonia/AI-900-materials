🚀 What Are Transformers?
- Transformers are deep learning models specialized for natural language processing (NLP) tasks like:
- Text classification (e.g., sentiment analysis)
- Summarization
- Semantic comparison
- Text generation (like ChatGPT!)

🧱 Transformer Architecture: Two Core Blocks
1. Encoder – Converts input text into semantic embeddings (numeric meaning).
2. Decoder – Uses those embeddings to generate meaningful output text.
  🧠 GPT (like GPT-4) uses only the decoder block
  🔍 BERT uses only the encoder block

🔤 Tokenization
Text is broken into smaller pieces (tokens), which can be:
- Words
- Subwords
- Characters

Example:
"I heard a dog" → {1, 2, 3, 4} (token IDs)

📊 Embeddings
- Tokens are mapped to vectors (multi-dimensional arrays) that encode meaning.
- Words with similar meanings get vectors pointing in similar directions.

Example:
- "dog": [10, 3, 2]
- "puppy": [5, 2, 1] → close to "dog"
- "skateboard": [-3, 3, 2] → very different

🎯 Attention (and Self-Attention)
- This is the key innovation of transformers:
- Self-attention lets the model decide which other words in the sentence matter when interpreting a word.
- Helps disambiguate meaning based on context:
  - "bark" (tree) vs. "bark" (dog)
- The model weighs the importance of each token in predicting the next one.

🔁 How It Generates Text
- Takes input like "The weather today is"
- Assigns attention weights to context
- Predicts next token (e.g., "sunny")
- Appends that token and repeats the process
- This continues until the model produces a full sentence (or hits a stopping point).

💡 Why Transformers Work So Well
- Trained on massive datasets
- Able to model context, not just word-by-word
- Embeddings and attention allow understanding of meaning and relationships
- Transformers like GPT-4 don’t "understand" in the human sense — but they’re incredibly effective at generating language that feels intelligent.
